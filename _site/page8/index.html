<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Latest Posts &#8211; Linuxdynasty</title>
<meta name="description" content="What I am up too.">
<meta name="keywords" content="Python, GoLang, automation, scripting, programming, vFense, metrics, monitoring">

<!-- Twitter Cards -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://linuxdynasty.github.io/Linuxdynasty/images/abstract-1.jpg">

<meta name="twitter:title" content="Latest Posts">
<meta name="twitter:description" content="What I am up too.">
<meta name="twitter:creator" content="@linuxdynasty">

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Latest Posts">
<meta property="og:description" content="What I am up too.">
<meta property="og:url" content="http://linuxdynasty.github.io/Linuxdynasty/page8/index.html">
<meta property="og:site_name" content="Linuxdynasty">





<link rel="canonical" href="http://linuxdynasty.github.io/Linuxdynasty/page8/">
<link href="http://linuxdynasty.github.io/Linuxdynasty/feed.xml" type="application/atom+xml" rel="alternate" title="Linuxdynasty Feed">
<link rel="author" href="https://google.com/+AllenSanabria?rel=author">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://linuxdynasty.github.io/Linuxdynasty/assets/css/main.min.css">
<!-- Webfonts -->
<link href="http://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://linuxdynasty.github.io/Linuxdynasty/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://linuxdynasty.github.io/Linuxdynasty/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://linuxdynasty.github.io/Linuxdynasty/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://linuxdynasty.github.io/Linuxdynasty/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://linuxdynasty.github.io/Linuxdynasty/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://linuxdynasty.github.io/Linuxdynasty/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://linuxdynasty.github.io/Linuxdynasty/images/apple-touch-icon-144x144-precomposed.png">



</head>

<body id="post-index" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://linuxdynasty.github.io/Linuxdynasty">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://linuxdynasty.github.io/Linuxdynasty/images/avatar.jpg" alt="Allen Sanabria photo" class="author-photo">
					<h4>Allen Sanabria</h4>
					<p>DevOps developer who loves programming, monitoring, automation, and metrics.</p>
				</li>
				<li><a href="http://linuxdynasty.github.io/Linuxdynasty/about/">Learn More</a></li>
				<li>
					<a href="mailto:asanabria <@> linuxdynasty dot org"><i class="fa fa-envelope"></i> Email</a>
				</li>
				<li>
					<a href="http://twitter.com/linuxdynasty"><i class="fa fa-twitter"></i> Twitter</a>
				</li>
				
				<li>
					<a href="https://google.com/+AllenSanabria"><i class="fa fa-google-plus"></i> Google+</a>
				</li>
				<li>
					<a href="http://linkedin.com/in/linuxdynasty"><i class="fa fa-linkedin"></i> LinkedIn</a>
				</li>
				<li>
					<a href="http://github.com/linuxdynasty"><i class="fa fa-github"></i> GitHub</a>
				</li>
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://linuxdynasty.github.io/Linuxdynasty/posts/">All Posts</a></li>
				<li><a href="http://linuxdynasty.github.io/Linuxdynasty/tags/">All Tags</a></li>
			</ul>
		</li>
		<li><a href="http://linuxdynasty.github.io/Linuxdynasty"></a></li><li><a href="http://linuxdynasty.github.io/Linuxdynasty"></a></li>
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->


<div class="entry-header">
  <div class="image-credit">Image source: <a href="http://www.dargadgetz.com/ios-7-abstract-wallpaper-pack-for-iphone-5-and-ipod-touch-retina/">dargadgetz</a></div><!-- /.image-credit -->
  
    <div class="entry-image">
      <img src="http://linuxdynasty.github.io/Linuxdynasty/images/abstract-1.jpg" alt="Latest Posts">
    </div><!-- /.entry-image -->
  
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>Linuxdynasty</h1>
      <h2>Latest Posts</h2>
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->

<div id="main" role="main">
  
<article class="hentry">
  <header>
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2009-08-24T01:25:22-04:00"><a href="http://linuxdynasty.github.io/Linuxdynasty/clustering/howto-setup-a-quorum-disk/">August 24, 2009</a></time></span><span class="author vcard"><span class="fn"><a href="http://linuxdynasty.github.io/Linuxdynasty/about/" title="About Allen Sanabria">Allen Sanabria</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://linuxdynasty.github.io/Linuxdynasty/clustering/howto-setup-a-quorum-disk/#disqus_thread">Comment</a></span>
      
      <span class="entry-reading-time pull-right">
        <i class="fa fa-clock-o"></i>
        
        Reading time ~5 minutes
      </span><!-- /.entry-reading-time -->
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://linuxdynasty.github.io/Linuxdynasty/clustering/howto-setup-a-quorum-disk/" rel="bookmark" title="HowTo setup a Quorum Disk" itemprop="url">HowTo setup a Quorum Disk</a></h1>
    
  </header>
  <div class="entry-content">
    <p>Today's tutorial will be on the infamous Quorum disk.  When I first setup my <a title="" href="http://www.linuxdynasty.org/howto-setup-gfs2-with-clustering.html">GFS2 shared Cluster of 3 nodes</a>, I was quite impressed with the fact that 3 nodes were sharing the same file system.  Now that everything was up and running, I wanted to see what would happen if I brought down, 2 out of the 3 nodes in the cluster. I turned off 1st node and all was well, I was still able to access my GFS2 mount on the other 2 nodes. Then I decided to reboot the 2nd node, and guess what happened???? QUORUM DISSOLVED!!! Now on my final node the GFS2 file system was still mounted but I could not touch a file or run a ls on the mount... It just hung there!!</p>
<p>Well I knew this was not going to be acceptable.... Since if I still have 1 node available, the node should still be able to use the GFS2 mount. So I did some research about this quorum disk and what it can do for me. Let me tell you, this was exactly what I was looking for. 1st let me start out my explaining what a quorum is ( relating to clustering ). A quorum is the minimal number of votes that is needed in a cluster, usually the majority. So if you have 3 nodes in a cluster, that means you have a total of 3 votes in the cluster and you will need a minimum of 2 votes to remain in a quorate state. Which means you can lose 1 node in the cluster and the other nodes are still functional. But if you lose 2 nodes, your quorum will be dissolved. Which means even though your GFS2 file system is still mounted on your final node, it will not be accessible to you.</p>
<p>The quorum disk will help this particular situation... You ask how???? Well here it is...  Qdisk needs at a minimum of a 10MB disk partition shared across the cluster.<br />
Qdiskd runs on each node in the cluster, periodically checking its<br />
own health and then placing its state information into its assigned<br />
portion of the shared disk. On each node qdiskd then looks at the state of<br />
the other nodes in the cluster as posted in their area of the qdisk<br />
partition. When all the nodes that are running qdiskd are in a healthy state, the quorum of the cluster is increased by the value of the shared Quorum Disk.</p>
<p><span class="attention">The Value of the Quorum Disk should be n-1 ( Number of nodes - 1 ). In this case the Value should be 2 ( 3 -1 ).</span></p>
<p><a id="more"></a><a id="more-217"></a></p>
<p>If on a particular node, qdisk is unable to access its shared disk<br />
area after several attempts. The qdiskd running on another node in<br />
the cluster will request that the node which has issues communicating, to be fenced. Now that you have the basic understanding of what Qdisk is, I will now show you how to set it up.</p>
<ol>
<li>You will need a shared volume like ( iscsi, Fiber, VMware Shared Disk, etc.. )  that can be accessed by all nodes in the cluster. In this tutorial I am using a shared vmdk, and on all my nodes that is /dev/sdc. I will be using the 1st partition on /dev/sdc
<pre>fdisk -l /dev/sdc

Disk /dev/sdc: 5368 MB, 5368709120 bytes255 heads, 63 sectors/track, 652 cylindersUnits = cylinders of 10065 * 512 = 8225280 bytes

   Device Boot      Start         End      Blocks   Id  System/dev/sdc1               1           2       16033+  83  Linux/dev/sdc2               3         652     5221125   83  Linux</pre>
<p>&nbsp;
<li>Once the volumes are accessible from all the nodes, you will now need to create the Quorum Disk.example below...
<pre>mkqdisk -c /dev/sdc1 -l testQdisk

mkqdisk v0.6.0Writing new quorum disk label 'testQdisk' to /dev/sdc1.WARNING: About to destroy all data on /dev/sdc1; proceed [N/y] ? yWarning: Initializing previously initialized partitionInitializing status block for node 1...Initializing status block for node 2...Initializing status block for node 3...Initializing status block for node 4...Initializing status block for node 5...Initializing status block for node 6...Initializing status block for node 7...Initializing status block for node 8...Initializing status block for node 9...Initializing status block for node 10...Initializing status block for node 11...Initializing status block for node 12...Initializing status block for node 13...Initializing status block for node 14...Initializing status block for node 15...Initializing status block for node 16...</pre>
</li>
<li>Now we need to verify that the quorum disk has been initialized correctly. You should run the blow command on all 3 nodes, so that you have a piece of mind.... Example below..
<pre>mkqdisk -L

mkqdisk v0.6.0/dev/disk/by-path/pci-0000:00:11.0-scsi-0:0:1:0-part1:/dev/sdc1:        Magic:                eb7a62c2        Label:                testQdisk        Created:              Mon Aug 24 10:20:25 2009        Host:                 gfs1        Kernel Sector Size:   512        Recorded Sector Size: 512</pre>
</li>
<li>We now need to add the qdisk information into /etc/cluster/cluster.conf after the  &lt;/clusternodes&gt; but before we do that, lets make a backup of cluster.conf... .example below
<pre>cp /etc/cluster/cluster.conf /etc/cluster.conf.orig</pre>
<p>Now we edit cluster.conf</p>
<pre>        &lt;/clusternodes&gt;        &lt;quorumd interval="3" tko="23" votes="2" label="testQdisk"&gt;        &lt;/quorumd&gt;</pre>
</li>
<li>Once you added the above info into cluster.conf, you should also increase the version number by 1. Example below..
<pre>&lt;cluster config_version="31" name="MyCluster"&gt;</pre>
</li>
<li>Now to verify that your config is correct, run ccs_tool update /etc/cluster/cluster.conf. If you get no errors from ccs_tool, you are now able to proceed to the next step.</li>
<li>You will then copy the new /etc/cluster/cluster.conf to the other 2 nodes in the cluster.
<pre>scp /etc/cluster/cluster.conf gfs2:/etc/clusterscp /etc/cluster/cluster.conf gfs3:/etc/cluster</pre>
</li>
<li>Now restart the CMAN daemon on all 3 nodes like so "service cman restart". Now wait a minutes or so and run clustat on all 3 nodes, you will notice that a quorum disk entry should popup on the bottom of the list like so..
<pre>clustatCluster Status for MyCluster @ Mon Aug 24 10:40:47 2009Member Status: Quorate

 Member Name                                                     ID   Status ------ ----                                                     ---- ------ gfs1                                                            1 Online, Local gfs2                                                            2 Online gfs3                                                            3 Online /dev/disk/by-path/pci-0000:00:11.0-scsi-0:0:1:0-part1           0 Offline, Quorum Disk</pre>
<p>Or you can taill -f /var/log/messages</p>
<p><small>Aug 24 10:39:32 gfs3 qdiskd[14488]: &lt;info&gt; Quorum Partition: /dev/disk/by-path/pci-0000:00:11.0-scsi-0:0:3:0-part1 Label: testQdisk Aug 24 10:39:32 gfs3 qdiskd[14489]: &lt;info&gt; Quorum Daemon Initializing Aug 24 10:40:47 gfs3 qdiskd[14489]: &lt;info&gt; Initial score 1/1 Aug 24 10:40:47 gfs3 qdiskd[14489]: &lt;info&gt; Initialization complete Aug 24 10:40:47 gfs3 openais[2561]: [CMAN ] quorum device registered Aug 24 10:40:47 gfs3 qdiskd[14489]: &lt;notice&gt; Score sufficient for master operation (1/1; required=1); upgrading </small>&nbsp;
<li>We are pretty much done now. All we need to do now is verify that our total votes went from 3 to 5 and our expected votes went from 2 to 3..
<pre>cman_tool status

Version: 6.1.0Config Version: 32Cluster Name: MyClusterCluster Id: 46516Cluster Member: YesCluster Generation: 384Membership state: Cluster-MemberNodes: 3Expected votes: 3Quorum device votes: 2Total votes: 5Quorum: 3  Active subsystems: 10Flags: Dirty Ports Bound: 0 11 177  Node name: gfs1Node ID: 1Multicast addresses: 239.192.181.106 Node addresses: 192.168.101.100</pre>
</li>

<p>Well that was not that bad.... Was it??? If all went well you can now have the GFS2 shared file system mounted on either 1 or 3 nodes and it will still be accessible by your cluster. Also your cluster will still be quorate, even if you are down to 1 node in that cluster.</p>
</p></li></p></li></ol>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2009-08-14T17:05:15-04:00"><a href="http://linuxdynasty.github.io/Linuxdynasty/clustering/howto-increase-gfs2-performance-in-a-cluster/">August 14, 2009</a></time></span><span class="author vcard"><span class="fn"><a href="http://linuxdynasty.github.io/Linuxdynasty/about/" title="About Allen Sanabria">Allen Sanabria</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://linuxdynasty.github.io/Linuxdynasty/clustering/howto-increase-gfs2-performance-in-a-cluster/#disqus_thread">Comment</a></span>
      
      <span class="entry-reading-time pull-right">
        <i class="fa fa-clock-o"></i>
        
        Reading time ~2 minutes
      </span><!-- /.entry-reading-time -->
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://linuxdynasty.github.io/Linuxdynasty/clustering/howto-increase-gfs2-performance-in-a-cluster/" rel="bookmark" title="HowTo Increase GFS2 Performance in a Cluster" itemprop="url">HowTo Increase GFS2 Performance in a Cluster</a></h1>
    
  </header>
  <div class="entry-content">
    <p>In the last HowTo, I showed you how to setup GFS2 file system with Red Hat Clustering. I will now show you how to optimize the performance of your GFS2 mounts. The gfs_controld daemon manages the mounting, unmounting, and the recovery of the GFS2 mounts. gfs_controld also manages the posix lock.</p>
<p>By default  the plock_rate_limit option is set to 100. This will allow a maximum of 100 locks per second, which will decrease your GFS2 performance. See below...</p>
<p><strong> &lt;dlm plock_ownership="0" plock_rate_limit="100"/&gt;<br />
&lt;gfs_controld plock_rate_limit="100"/&gt;</strong></p>
<p>You can test the performance of you cluster by downloading the program <strong><a title="" href="http://wiki.samba.org/index.php/Ping_pong">ping_pong.c.</a></strong> This program was very helpful to me in debugging the poor performance in my GFS2 cluster.<br />
The instructions on how to compile the program and run it is on the site http://wiki.samba.org/index.php/Ping_pong.When I initially ran ping_pong, I only got a max of 97 plocks per second. After removing the rate limit I was able to get about 3000 Plocks per second.</p>
<p><a id="more"></a><a id="more-216"></a></p>
<p>You should change the plock_rate_limit to 0 and the dlm plock_ownership to 1. See below..</p>
<p><strong> &lt;dlm plock_ownership="1" plock_rate_limit="0"/&gt;<br />
&lt;gfs_controld plock_rate_limit="0"/&gt;</strong></p>
<p>FYI.. The settings above are set in <strong>/etc/cluster/cluster.conf</strong>   Example below...<br />
<strong>    &lt;cman/&gt;<br />
&lt;dlm plock_ownership="1" plock_rate_limit="0"/&gt;<br />
&lt;gfs_controld plock_rate_limit="0"/&gt;</strong><br />
<strong>&lt;/cluster&gt;</strong></p>
<p>My settings were added to the end of cluster.conf.<br />
<span class="attention">After adding the above settings, please reboot all the nodes for the settings to take affect. </span></p>
<p>The above will increase locking operations at the cost of an increase in network utilization. You also can increase the performance of your GFS2 mount by mounting it with these options ( <strong>noatime</strong> and <strong>nodiratime</strong> )... Example below..</p>
<p><strong>mount -o noatime,nodiratime, /dev/mapper/mytest_gfs2-MyGFS2test /GFS/</strong></p>
<p>Another way to tune GFS2 directly, is by decreasing how often GFS2 demotes its locks.  Demote_secs is the number of seconds that gfsd will wake up and demote locks and flush data to disk. The default is set to 300 seconds which is equal to 5 minutes. I currently have mine to demote every 20 seconds, and believe you me... I saw a big performance increase.</p>
<p><strong>gfs2_tool settune /GFS demote_secs 20</strong></p>
<p>I chose 20 seconds, but it does not mean that is what you need to chose. You can play with those numbers and see how performance either increases or decreases. The option needs to be set every time the file system is mounted. So you might want to add this option in rc.local or in the gfs2 startup script at the end.</p>
<p><strong> echo "gfs2_tool settune /GFS demote_secs 20" &gt;&gt; /etc/rc.local</strong></p>
<p>When all was said and done I was able to get over 3000 plocks per sec after the optimization was done and my file level operations drastically increased in performance. I Hope the above helps you the way it helped me.</p>
<p><span class="attention">Very Important.... Make sure that updatedb does not run on your GFS2 mounts.. This will kill your GFS2 mount!!!! </span></p>
<p>&nbsp;</p>
<p>&nbsp;</p>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2009-08-14T16:48:49-04:00"><a href="http://linuxdynasty.github.io/Linuxdynasty/clustering/howto-setup-gfs2-with-clustering/">August 14, 2009</a></time></span><span class="author vcard"><span class="fn"><a href="http://linuxdynasty.github.io/Linuxdynasty/about/" title="About Allen Sanabria">Allen Sanabria</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://linuxdynasty.github.io/Linuxdynasty/clustering/howto-setup-gfs2-with-clustering/#disqus_thread">Comment</a></span>
      
      <span class="entry-reading-time pull-right">
        <i class="fa fa-clock-o"></i>
        
        Reading time ~4 minutes
      </span><!-- /.entry-reading-time -->
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://linuxdynasty.github.io/Linuxdynasty/clustering/howto-setup-gfs2-with-clustering/" rel="bookmark" title="HowTo setup GFS2 with Clustering" itemprop="url">HowTo setup GFS2 with Clustering</a></h1>
    
  </header>
  <div class="entry-content">
    <p>In my last project at work, I had to replace NFS with GFS2 and Clustering. So in this tutorial I will show you how to create a Red Hat or CentOS cluster with GFS2. I will also show you how to optimize GFS2 performance in the next HowTo, because you will quickly notice some loss of performance until you do a little optimization first.I will 1st show you how do build a Cluster with GFS2 on the Command Line and in the next tutorial I will show you how to do the same thing using Conga.</p>
<p>In this tutorial I am using 3 CentOS Virtual Machines running CentOS 5.3 in VMware ESX 3.5. For the GFS2 File System I am using a vmdk built with the thick option, that is shared among all the Virtual Machines. You also can use iscsi or fiber... This option is up to you.</p>
<p><span class="attention">Always make sure your iptables (If you know the port's and protocols for clustering, then add it to iptables ) and selinux is OFF. If not you will run into issues.</span></p>
<p>The 3 machines I am using are called</p>
<ul>
<li>gfs1 == 192.168.101.100</li>
<li>gfs2 == 192.168.101.101</li>
<li>gfs3 == 192.168.101.103</li>
</ul>
<p><a id="more"></a><a id="more-215"></a></p>
<p>Since I'm using VMware ESX for the 3 machines above I will also be using vmware for fencing. The info is below for my test setup</p>
<ul>
<li>ESX Host Name == esxtest<br />
ESX IP Address == 192.168.101.50</li>
<li>ESX user login info below<br />
login == esxuser<br />
password == esxpass</li>
<li>ESX admin login info below<br />
login == root<br />
password == esxpass</li>
</ul>
<p>&nbsp;</p>
<p>The 1st command you need to know for creating and modifying your cluster is the '<strong>ccs_tool</strong>' command.</p>
<p>Below I will show you the necessary steps to create a cluster and then the GFS2 filesystem</p>
<ol>
<li>First step is to install the necessary RPM's..<br />
<strong>yum -y install modcluster rgmanager gfs2 gfs2-utils lvm2-cluster cman</strong></li>
<li>Second step is to create a cluster on gfs1<br />
<strong>ccs_tool create GFStestCluster</strong></li>
<li>Now that the cluster is created, we will now need to add the fencing devices.<br />
( For simplicity you can just use fence_manual for each host.. <strong>ccs_tool addfence -C gfs1_ipmi fence_manual</strong> )<br />
But if you are using VMware ESX like I am you should use fence_vmware like so...<br />
<strong>ccs_tool addfence -C gfs1_vmware fence_vmware ipaddr=esxtest login=esxuser passwd=esxpass vmlogin=root vmpasswd=esxpass port="/vmfs/volumes/49086551-c64fd83c-0401-001e0bcd6848/eagle1/gfs1.vmx"</strong><br />
<strong>ccs_tool addfence -C gfs2_vmware fence_vmware ipaddr=esxtest login=esxuser passwd=esxpass vmlogin=root vmpasswd=esxpass port="vmfs/volumes/49086551-c64fd83c-0401-001e0bcd6848/gfs2/gfs2.vmx"</strong><br />
<strong>ccs_tool addfence -C gfs3_vmware fence_vmware ipaddr=esxtest login=esxuser passwd=esxpass vmlogin=root vmpasswd=esxpass port="/vmfs/volumes/49086551-c64fd83c-0401-001e0bcd6848/gfs3/gfs3.vmx"</strong></li>
<li>Now that we added the Fencing devices, it is time to add the nodes..<br />
<strong>ccs_tool addnode -C gfs1 -n 1 -v 1 -f gfs1_vmware<br />
ccs_tool addnode -C gfs2 -n 2 -v 1 -f gfs2_vmware<br />
ccs_tool addnode -C gfs3 -n 3 -v 1 -f gfs3_vmware</strong></li>
<li>Now we need to copy this configuration over to the other 2 nodes from gfs1 or we can run the exact same commands above on the other 2 nodes..<br />
<strong>scp /etc/cluster/cluster.conf root@gfs2:/etc/cluster/cluster.conf<br />
scp /etc/cluster/cluster.conf root@gfs3:/etc/cluster/cluster.conf</strong></li>
<li>You can verify the config on all 3 nodes by running the following commands below..<br />
<strong>ccs_tool lsnode<br />
ccs_tool lsfence</strong></li>
<li>You are ready to proceed with starting up the following daemons on all the nodes in the cluster, once you either copied over the configs or re ran the same commands above on the other 2 nodes<br />
<strong>/etc/init.d/cman start<br />
/etc/init.d/rgmanager start</strong></li>
<li>You can now check the status of your cluster by running the commands below...<strong><br />
clustat<br />
cman_tool status</strong></li>
<li>If you want to test the vmware fencing you can do so by doing the following..<strong> ( </strong>run the command below on the 1st node and use the 2nd node as the node to be fenced<strong> )<br />
fence_vmware -a esxtest -l esxuser -p esxpass -L root -P esxpass -n "/vmfs/volumes/49086551-c64fd83c-0401-001e0bcd6848/gfs2/gfs2.vmx" -v<br />
</strong></li>
<li>Before we start to create the LVM2 volumes and Proceed to GFS2, we will need to enable clustering in LVM2.<br />
<strong>lvmconf --enable-cluster</strong></li>
<li>Now it is time to create the LVM2 Volumes...<br />
<strong>pvcreate MyTestGFS /dev/sdb<br />
vgcreate -c y mytest_gfs2 /dev/sdb<br />
lvcreate -n MyGFS2test -L 5G mytest_gfs2<br />
/etc/init.d/clvmd start</strong></li>
<li>You should now also start <strong>clvmd</strong> on the other 2 nodes..<strong><br />
</strong></li>
<li>Once the above has been completed, you will now need to create the GFS2 file system.. Example below..<br />
mkfs -t &lt;filesystem&gt; -p &lt;locking mechanism&gt; -t &lt;ClusterName&gt;:&lt;PhysicalVolumeName&gt; -j &lt;JournalsNeeded == amount of nodes in cluster&gt; &lt;location of filesystem&gt;<br />
<strong>mkfs -t gfs2 -p lock_dlm -t MyCluster:MyTestGFS -j 4 /dev/mapper/mytest_gfs2-MyGFS2test</strong></li>
<li>All we need to do on the 3 nodes, is to mount the GFS2 file system.<br />
<strong>mount /dev/mapper/mytest_gfs2-MyGFS2test /mnt/<br />
</strong></li>
<li>Once you mounted your GFS2 file system You can the following commands..<strong><br />
gfs2_tool list<br />
gfs2_tool df </strong></li>
</ol>
<p>Now it is time to wrap it up with some final commands...</p>
<ol>
<li>Now that we have a fully functional cluster and a mountable GFS2 file system, we need to make sure all the necessary daemons start up with the cluster..<br />
<strong>chkconfig --level 345 rgmanager on<br />
chkconfig --level 345 clvmd on<br />
chkconfig --level 345 cman on<br />
chkconfig --level 345 gfs2 on</strong></li>
<li>If you want the GFS2 file system to be mounted at startup you can add this to /etc/fstab..<br />
<strong>echo "/dev/mapper/mytest_gfs2-MyGFS2test /GFS gfs2 defaults,noatime,nodiratime 0 0" &gt;&gt; /etc/fstab</strong></li>
</ol>
<p>In the next up coming tutorials I will show you how to do the same as above but with the Red Hat Conga gui and I will also show you how to optimize your GFS2 Cluster setup.</p>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2009-07-14T12:18:45-04:00"><a href="http://linuxdynasty.github.io/Linuxdynasty/how-to-install-the-data-protector-6-linux-install-server-on-linux/">July 14, 2009</a></time></span><span class="author vcard"><span class="fn"><a href="http://linuxdynasty.github.io/Linuxdynasty/about/" title="About Allen Sanabria">Allen Sanabria</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://linuxdynasty.github.io/Linuxdynasty/how-to-install-the-data-protector-6-linux-install-server-on-linux/#disqus_thread">Comment</a></span>
      
      <span class="entry-reading-time pull-right">
        <i class="fa fa-clock-o"></i>
        
        Reading time ~1 minute
      </span><!-- /.entry-reading-time -->
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://linuxdynasty.github.io/Linuxdynasty/how-to-install-the-data-protector-6-linux-install-server-on-linux/" rel="bookmark" title="How To install the Data Protector 6.+ Linux Install Server on Linux" itemprop="url">How To install the Data Protector 6.+ Linux Install Server on Linux</a></h1>
    
  </header>
  <div class="entry-content">
    <p>Now for those of you who are like me and really do not like reading<br />
long winded PDF Installation guides. Here is the SHORT on how to get<br />
Data Protector Installation server installed on RedHat/CentOS/Suse Linux System.</p>
<p user="true" style="display: none" mce_style="display:none">&nbsp;</p>
<ol>
<li>Download the 2 Data Protector Linux Installer ISO Images from HP's WebSite.</li>
<li>Mount both images on the server you are installing this on<br />
Example... ( <strong>mount -o loop -t iso9660 /root/B6960-10011.iso /mnt1/</strong> )<br />
Example... ( <strong>mount -o loop -t iso9660 /root/B6960-10012.iso /mnt2/</strong> )</li>
<li>Now copy over the contents of DP_DEPOT and LOCAL_INSTALL to a local mount of disc1<br />
Example... ( <strong>mkdir /root/DPINSTALL</strong> )<br />
Example... ( <strong>cp -rf /mnt1/DP_DEPOT /root/DPINSTALL</strong>/</li>
<li>Now copy over the contents of DP_DEPOT from disc2 into /root/DPINSTALL/DP_DEPOT/<br />
Example... ( <strong>cp -rf /mnt2/DP_DEPOT/* DP_DEPOT/</strong> )&nbsp;&nbsp; You will be overiding 2 rpm's, but that is alright</li>
<li>Change over to the DPINSTALL directory and install the RPM's<br />
Example... ( <strong>cd /root/DPINSTALL</strong> )<br />
Example... ( <strong>rpm -ivh *.rpm</strong> )</li>
<li>Almost done..... :)</li>
<li>I'm<br />
assuming you will install clients using SSH Keys. And this is how I<br />
will be showing you on how to get this to work. You can copy over the<br />
example omnirc file and use that one..<br />
Example... ( <strong>cp /opt/omni/.omnirc.TMPL&nbsp; /opt/omni/.omnirc</strong> )</li>
<li>Now you will need to enable the ssh keys option.<br />
Example.... ( open up /opt/omni/.omnirc using your favorite editor ) and modify this line....<br />
<strong>#&nbsp; OB2_SSH_ENABLED=0|1</strong><br />
and change it to....<br />
<strong>OB2_SSH_ENABLED=1</strong></li>
<li>We now need to generate a SSH KEY<br />
Example... ( <strong>ssh-keygen -t rsa -b 2048</strong> )</li>
<li>Once the key is generated ( assuming with&nbsp; out a password ) you now need to copy that key over to the clients that you want Data Protector installed on.<br />
Example... ( <strong>scp /root/.ssh/id_rsa.pub client:/root/.ssh/authorized_keys</strong> ) This is assuming that you have a /root/.ssh/ directory on your clients. If not you will have to create one.
</li>
</ol>
<p user="true" style="display: none" mce_style="display:none">&nbsp;</p>
<p>You are now ready to import the installtion server into your Data<br />
Protector Cell Manager... <span class="attention">VERY IMPORTANT &quot;&quot;&quot;&quot; DNS or your HOST entries<br />
for your Installation Server/Cell Manager/ and all your Clients has to<br />
be correct &quot;&quot;&quot;&quot;</span></p>
<p>&nbsp;</p>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2009-07-06T15:06:37-04:00"><a href="http://linuxdynasty.github.io/Linuxdynasty/how-to-automate-the-install-of-vmware-tools-after-any-kernel-update/">July 06, 2009</a></time></span><span class="author vcard"><span class="fn"><a href="http://linuxdynasty.github.io/Linuxdynasty/about/" title="About Allen Sanabria">Allen Sanabria</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://linuxdynasty.github.io/Linuxdynasty/how-to-automate-the-install-of-vmware-tools-after-any-kernel-update/#disqus_thread">Comment</a></span>
      
      <span class="entry-reading-time pull-right">
        <i class="fa fa-clock-o"></i>
        
        Reading time ~1 minute
      </span><!-- /.entry-reading-time -->
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://linuxdynasty.github.io/Linuxdynasty/how-to-automate-the-install-of-vmware-tools-after-any-kernel-update/" rel="bookmark" title="How To automate the install of vmware-tools after any kernel update" itemprop="url">How To automate the install of vmware-tools after any kernel update</a></h1>
    
  </header>
  <div class="entry-content">
    <p>The other day I was installing kernel updates on a few of my Red Hat servers and I ran into a minor nuisance.&nbsp; After each reboot, I no longer had network connectivity on the hosts, that has the updated kernel. I then realized, that vmware-tools was not running on all of those hosts. So after a few manual instance of me running &quot;<strong>vmware-tools-config.pl -d</strong>&quot; and &quot;<strong>/etc/init.d/network restart</strong>&quot;....I decided to automate that, by adding the below into &quot;<strong>/etc/rc.local</strong>&quot; </p>
<pre>rkernel=`uname -r`<br />if [ -e /etc/vmware-tools/not_configured ]; then<br />    echo &quot;vmware-tools not configured for running kernel $rkernel&quot;<br />    echo &quot;running vmware-config-tools.pl&quot;<br />    /usr/bin/vmware-config-tools.pl -d<br />    echo &quot;vmware-tools now compiled for running kernel $rkernel&quot;<br />    echo &quot;restarting networking&quot;<br />    /etc/init.d/network restart<br />    echo &quot;network restarted&quot;<br />    exit 0<br />fi<br />&nbsp;</pre>
<p>&nbsp;After each host rebooted, I now had network connectivity and vmware-tools was running. I hope the above will save you some time. </p>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->


<div class="pagination">
  
    
      <a href="http://linuxdynasty.github.io/Linuxdynasty/page7" class="btn">Previous</a>
    
  
  <ul class="inline-list">
    <li>
      
        <a href="http://linuxdynasty.github.io/Linuxdynasty">1</a>
      
    </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page2">2</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page3">3</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page4">4</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page5">5</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page6">6</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page7">7</a>
        
      </li>
    
      <li>
        
          <span class="current-page">8</span>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page9">9</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page10">10</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page11">11</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page12">12</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page13">13</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page14">14</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page15">15</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page16">16</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page17">17</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page18">18</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page19">19</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page20">20</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page21">21</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page22">22</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page23">23</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page24">24</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page25">25</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page26">26</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page27">27</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page28">28</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page29">29</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page30">30</a>
        
      </li>
    
      <li>
        
          <a href="http://linuxdynasty.github.io/Linuxdynasty/page31">31</a>
        
      </li>
    
  </ul>
  
    <a href="http://linuxdynasty.github.io/Linuxdynasty/page9" class="btn">Next</a>
  
</div><!-- /.pagination -->

</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2014 Allen Sanabria. Powered by <a href="http://jekyllrb.com">Jekyll</a> using the <a href="http://mademistakes.com/hpstr/">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://linuxdynasty.github.io/Linuxdynasty/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://linuxdynasty.github.io/Linuxdynasty/assets/js/scripts.min.js"></script>

<!-- Asynchronous Google Analytics snippet -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-2333243-1', 'auto');  
  ga('require', 'linkid', 'linkid.js');
  ga('send', 'pageview');
</script>

          

</body>
</html>